{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7b4303cf-dd90-46ae-b7b8-a9ed45d2dd80",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Maintenance CSV\n",
    "maintenance_data = [\n",
    "    [1,\"2025-08-01\",\"Engine A\",\"Oil leakage\",\"Replace oil seal and check pressure\"],\n",
    "    [2,\"2025-08-02\",\"Landing Gear B\",\"Hydraulic failure\",\"Inspect hydraulic lines and refill fluid\"],\n",
    "    [3,\"2025-08-03\",\"Fuselage C\",\"Corrosion\",\"Apply anti-corrosion treatment and inspect panels\"],\n",
    "    [4,\"2025-08-04\",\"Avionics D\",\"Navigation error\",\"Run diagnostic and update firmware\"],\n",
    "    [5,\"2025-08-05\",\"Engine E\",\"Overheating\",\"Replace thermostat and clean cooling system\"]\n",
    "]\n",
    "df_maintenance = pd.DataFrame(maintenance_data, columns=[\"id\",\"date\",\"equipment\",\"issue\",\"procedure\"])\n",
    "df_maintenance.to_csv(\"maintenance.csv\", index=False)\n",
    "\n",
    "# Aircraft Taxonomy CSV\n",
    "taxonomy_data = [\n",
    "    [1,\"Engine\",\"Turbofan\",\"High-bypass turbofan engine used in commercial jets\"],\n",
    "    [2,\"Landing Gear\",\"Main Gear\",\"Retractable main landing gear assembly\"],\n",
    "    [3,\"Fuselage\",\"Body\",\"Pressurized aircraft body structure\"],\n",
    "    [4,\"Avionics\",\"Navigation\",\"Navigation system including GPS and INS\"],\n",
    "    [5,\"Electrical\",\"Power\",\"Electrical power distribution system\"],\n",
    "    [6,\"Hydraulics\",\"Actuation\",\"Hydraulic system controlling flaps and landing gear\"],\n",
    "    [7,\"Fuel\",\"Storage\",\"Fuel tanks and fuel distribution system\"],\n",
    "    [8,\"Cabin\",\"Seats\",\"Passenger seating and cabin layout\"],\n",
    "    [9,\"Flight Controls\",\"Elevators\",\"Primary control surfaces for pitch\"],\n",
    "    [10,\"Flight Controls\",\"Ailerons\",\"Primary control surfaces for roll\"]\n",
    "]\n",
    "df_taxonomy = pd.DataFrame(taxonomy_data, columns=[\"id\",\"category\",\"subcategory\",\"description\"])\n",
    "df_taxonomy.to_csv(\"aircrafttaxonomy.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "590a345b-4a56-4ef7-88dc-5192ef7ff731",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Ingestion pipeline to load data\n",
    "import os\n",
    "import json\n",
    "import pandas as pd\n",
    "import requests\n",
    "import httpx\n",
    "from sqlalchemy import create_engine, text\n",
    "from langchain.docstore.document import Document\n",
    "from langchain_postgres.vectorstores import PGVector\n",
    "\n",
    "# -----------------------------\n",
    "# Load services from env\n",
    "# -----------------------------\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "# -----------------------------\n",
    "# Embedding service details\n",
    "# -----------------------------\n",
    "def is_embeddingservice(service):\n",
    "    return service[\"name\"] == \"prod-embedding-nomic-text\"\n",
    "\n",
    "embedding_services = filter(is_embeddingservice, services[\"genai\"])\n",
    "embedding_credentials = list(embedding_services)[0][\"credentials\"]\n",
    "\n",
    "api_base = embedding_credentials[\"api_base\"] + \"/v1\"\n",
    "api_key = embedding_credentials[\"api_key\"]\n",
    "model_name = embedding_credentials[\"model_name\"]\n",
    "\n",
    "print(\"Embedding model:\", model_name)\n",
    "\n",
    "# -----------------------------\n",
    "# Database connection\n",
    "# -----------------------------\n",
    "def is_vectordbservice(service):\n",
    "    return service[\"name\"] == \"vector-db\"\n",
    "\n",
    "db_services = filter(is_vectordbservice, services[\"postgres\"])\n",
    "db_credentials = list(db_services)[0][\"credentials\"]\n",
    "db_uri = db_credentials[\"uri\"]\n",
    "\n",
    "print(\"DB URI:\", db_uri)\n",
    "\n",
    "engine = create_engine(db_uri)\n",
    "\n",
    "# Test DB connection\n",
    "with engine.connect() as conn:\n",
    "    version = conn.execute(text(\"SELECT version();\")).fetchone()\n",
    "    print(\"Connected to:\", version[0])\n",
    "\n",
    "# -----------------------------\n",
    "# Embedding function (REST call)\n",
    "# -----------------------------\n",
    "url = api_base + \"/embeddings\"\n",
    "headers = {\"Content-Type\": \"application/json\", \"Authorization\": f\"Bearer {api_key}\"}\n",
    "\n",
    "def embed_text(text: str):\n",
    "    payload = {\"model\": \"nomic-embed-text\", \"input\": text}\n",
    "    resp = requests.post(url, headers=headers, json=payload, verify=False)\n",
    "    resp.raise_for_status()\n",
    "    return resp.json()[\"data\"][0][\"embedding\"]\n",
    "\n",
    "class CustomEmbeddings:\n",
    "    def embed_documents(self, texts): return [embed_text(t) for t in texts]\n",
    "    def embed_query(self, text): return embed_text(text)\n",
    "\n",
    "embedding = CustomEmbeddings()\n",
    "\n",
    "# -----------------------------\n",
    "# PGVector setup\n",
    "# -----------------------------\n",
    "vectorstore = PGVector(\n",
    "    embeddings=embedding,\n",
    "    connection=db_uri,\n",
    "    collection_name=\"maintenance_and_taxonomy\",\n",
    "    use_jsonb=True,\n",
    "    create_extension=True,       # will create pgvector extension if not exists\n",
    "    pre_delete_collection=True,  # clears old data on restart\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# Load maintenance.csv\n",
    "# -----------------------------\n",
    "def sanitize_metadata(metadata):\n",
    "    sanitized = {}\n",
    "    for k, v in metadata.items():\n",
    "        if isinstance(v, set):\n",
    "            sanitized[k] = list(v)\n",
    "        elif not isinstance(v, (str, int, float, bool, dict, list, type(None))):\n",
    "            sanitized[k] = str(v)\n",
    "        else:\n",
    "            sanitized[k] = v\n",
    "    return sanitized\n",
    "\n",
    "df_maintenance = pd.read_csv(\"maintenance.csv\")  # columns: id, date, equipment, issue, procedure\n",
    "\n",
    "docs_csv = [\n",
    "    Document(\n",
    "        page_content=f\"{row['equipment']}: {row['issue']} - {row['procedure']}\",\n",
    "        metadata=sanitize_metadata({\"id\": row[\"id\"], \"source\": \"maintenance.csv\"})\n",
    "    )\n",
    "    for _, row in df_maintenance.iterrows()\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Load aircrafttaxonomy.csv\n",
    "# -----------------------------\n",
    "df_taxonomy = pd.read_csv(\"aircrafttaxonomy.csv\")  # columns: id, category, subcategory, description\n",
    "\n",
    "docs_taxonomy = [\n",
    "    Document(\n",
    "        page_content=f\"{row['category']} / {row['subcategory']}: {row['description']}\",\n",
    "        metadata=sanitize_metadata({\"id\": row[\"id\"], \"source\": \"aircrafttaxonomy.csv\"})\n",
    "    )\n",
    "    for _, row in df_taxonomy.iterrows()\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Insert into vectorstore\n",
    "# -----------------------------\n",
    "all_docs = docs_csv + docs_taxonomy\n",
    "vectorstore.add_documents(all_docs)\n",
    "\n",
    "print(f\"✅ Inserted {len(all_docs)} documents into the vectorstore!\")\n",
    "\n",
    "# -----------------------------\n",
    "# Inspect DB\n",
    "# -----------------------------\n",
    "query = text(\"SELECT * FROM langchain_pg_collection LIMIT 5;\")\n",
    "print(pd.read_sql(query, engine))\n",
    "\n",
    "query2 = text(\"SELECT * FROM langchain_pg_embedding LIMIT 5;\")\n",
    "print(pd.read_sql(query2, engine))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d3b799bf-e784-4dea-9e0b-155396b9d2cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcap/tmp/ipykernel_6122/3516390421.py:48: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  result = qa.run(query)\n",
      "/home/vcap/app/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'genai-proxy.sys.tas-ndc.kuhn-labs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Okay, let's see. The user is asking which aircraft equipment has reported issues with hydraulic leaks. The context provided mentions \"Landing Gear B: Hydraulic failure - Inspect hydraulic lines and refill fluid\" and \"Hydraulics / Actuation: Hydraulic system controlling flaps and landing gear.\" \n",
      "\n",
      "So, the user's question is about equipment that had hydraulic leaks. The context specifically points to the landing gear's hydraulic system. The entry under \"Landing Gear B\" mentions hydraulic failure, which involves inspecting hydraulic lines and refilling fluid. Additionally, the hydraulics/actuation section mentions the system controlling flaps and landing gear. \n",
      "\n",
      "Wait, but the question is about hydraulic leaks. The context provided doesn't explicitly mention leaks, but rather hydraulic failure, which could include leaks. However, the answer might be the landing gear's hydraulic system. The user might be referring to the equipment listed in the context. Since the context only mentions landing gear B and hydraulics/actuation, the answer is likely the landing gear's hydraulic system. But I need to check if the context mentions leaks or just failures. The user's question is about \"hydraulic leaks,\" but the context says \"hydraulic failure.\" Maybe the answer is the landing gear's hydraulic system, as that's the only equipment mentioned in the context related to hydraulics. \n",
      "\n",
      "I should make sure not to assume beyond the given context. The answer should be based on the provided information. Since the context mentions \"Landing Gear B: Hydraulic failure\" and \"Hydraulics / Actuation: Hydraulic system controlling flaps and landing gear,\" the equipment with hydraulic issues is the landing gear's hydraulic system. Therefore, the answer is the landing gear's hydraulic system.\n",
      "</think>\n",
      "\n",
      "The aircraft equipment that has reported issues with hydraulic systems is the **landing gear's hydraulic system**. Specifically, the context mentions \"Landing Gear B: Hydraulic failure - Inspect hydraulic lines and refill fluid,\" indicating that hydraulic failures (which could include leaks) have been reported in this equipment. Additionally, the hydraulic system controlling both flaps and landing gear is highlighted as part of the hydraulics/actuation system. \n",
      "\n",
      "If \"hydraulic leaks\" specifically refer to fluid escape, the context does not explicitly mention leaks, but hydraulic failures (such as those involving lines or fluid refills) are noted. For precise information on leaks, further details would be required. \n",
      "\n",
      "**Answer:** The landing gear's hydraulic system has reported issues, including hydraulic failures that require inspecting lines and refilling fluid. The hydraulics/actuation system (which controls flaps and landing gear) is also involved. However, the context does not explicitly state \"leaks,\" only \"hydraulic failure.\"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "import json\n",
    "import httpx\n",
    "from openai import OpenAI\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from langchain.tools import tool\n",
    "from langchain_openai import OpenAIEmbeddings\n",
    "from datetime import date\n",
    "import warnings\n",
    "import ssl\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from openai import OpenAI\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=30.0)\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "\n",
    "llm = ChatOpenAI(temperature=0.9, model=chat_credentials[\"model_name\"], base_url=chat_credentials[\"api_base\"], api_key=chat_credentials[\"api_key\"], http_client=httpx_client)\n",
    "\n",
    "# Create a retriever from your vectorstore\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\":3})\n",
    "\n",
    "# Build a RetrievalQA chain\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=llm,\n",
    "    chain_type=\"stuff\",\n",
    "    retriever=retriever\n",
    ")\n",
    "\n",
    "# Ask a question\n",
    "query = \"Which aircraft equipment has reported issues with hydraulic leaks?\"\n",
    "result = qa.run(query)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d0d1cfc6-60f5-405c-a833-cddc705d95a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import httpx\n",
    "from openai import OpenAI as RawOpenAI\n",
    "from trulens.providers.openai import OpenAI as TruOpenAI\n",
    "\n",
    "class CustomOpenAI(TruOpenAI):\n",
    "    def __init__(self, model_engine: str, api_key: str, api_base: str, verify: bool = True):\n",
    "        # Create custom HTTP client (disable SSL if verify=False)\n",
    "        http_client = httpx.Client(http2=True, verify=verify, timeout=60.0)\n",
    "\n",
    "        # Wrap inside the official OpenAI client (valid for TruLens)\n",
    "        raw_client = RawOpenAI(\n",
    "            api_key=api_key,\n",
    "            base_url=api_base,\n",
    "            http_client=http_client\n",
    "        )\n",
    "\n",
    "        # Initialize TruOpenAI with proper OpenAI client\n",
    "        super().__init__(\n",
    "            model_engine=model_engine,\n",
    "            client=raw_client\n",
    "        )\n",
    "\n",
    "        # Store extras bypassing Pydantic setattr restrictions\n",
    "        object.__setattr__(self, \"raw_client\", raw_client)\n",
    "        object.__setattr__(self, \"_http_client\", http_client)\n",
    "        object.__setattr__(self, \"_model_engine\", model_engine)\n",
    "\n",
    "    # Direct API helpers\n",
    "    def chat(self, messages, model=None, **kwargs):\n",
    "        model = model or self._model_engine\n",
    "        return self.raw_client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=messages,\n",
    "            **kwargs\n",
    "        )\n",
    "\n",
    "    def embed(self, input, model=None, **kwargs):\n",
    "        model = model or self._model_engine\n",
    "        return self.raw_client.embeddings.create(\n",
    "            model=model,\n",
    "            input=input,\n",
    "            **kwargs\n",
    "        )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "bec34b4b-de9c-4d94-b1b7-6c849237301b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Status code: 404\n",
      "Response JSON: {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n"
     ]
    }
   ],
   "source": [
    "import httpx\n",
    "import json\n",
    "\n",
    "\n",
    "# Your custom endpoint\n",
    "url = chat_credentials[\"api_base\"] + \"/v1/responses\"\n",
    "\n",
    "# Your API key (or token)\n",
    "api_key = chat_credentials[\"api_key\"]\n",
    "\n",
    "# Example payload\n",
    "payload = {\n",
    "    \"model\": chat_credentials[\"model_name\"],\n",
    "    \"input\": \"Which aircraft equipment had hydraulic issues?\",\n",
    "    \"temperature\": 0\n",
    "}\n",
    "\n",
    "# Create a client that bypasses SSL verification\n",
    "with httpx.Client(verify=False, http2=True, timeout=60.0) as client:\n",
    "    headers = {\n",
    "        \"Authorization\": f\"Bearer {api_key}\",\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "    response = client.post(url, headers=headers, data=json.dumps(payload))\n",
    "\n",
    "# Print status and response\n",
    "print(\"Status code:\", response.status_code)\n",
    "try:\n",
    "    print(\"Response JSON:\", response.json())\n",
    "except Exception:\n",
    "    print(\"Raw response:\", response.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0d7fdf2d-41d6-44b2-8041-6e113fbcae6f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcap/tmp/ipykernel_6122/458173627.py:38: DeprecationWarning: Tru is deprecated, use TruSession instead.\n",
      "  tru = Tru()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.prompts.chat.ChatPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.prompts.chat.BaseChatPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.prompts.base.BasePromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict, PromptValue]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.chat.ChatPromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.string.StringPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.base.BasePromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict, PromptValue]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat._StringImageMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'> for base <class 'langchain_core.prompts.message.BaseMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.SystemMessagePromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.string.StringPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.base.BasePromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict, PromptValue]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.prompts.chat._StringImageMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.prompts.message.BaseMessagePromptTemplate'>\n",
      "instrumenting <class 'langchain_core.prompts.chat.HumanMessagePromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_openai.chat_models.base.ChatOpenAI'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_openai.chat_models.base.BaseChatOpenAI'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_core.language_models.chat_models.BaseChatModel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_core.language_models.base.BaseLanguageModel[BaseMessage]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_core.language_models.base.BaseLanguageModel'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_core.runnables.base.RunnableSerializable[Union[PromptValue, str, Sequence[Union[BaseMessage, list[str], tuple[str, str], str, dict[str, Any]]]], TypeVar]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_openai.chat_models.base.ChatOpenAI'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.string.StrOutputParser'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.transform.BaseTransformOutputParser[str]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.transform.BaseTransformOutputParser'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.output_parsers.base.BaseOutputParser'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.runnables.base.RunnableSerializable[Union[BaseMessage, str], TypeVar]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.output_parsers.string.StrOutputParser'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain.chains.llm.LLMChain'> for base <class 'langchain.chains.llm.LLMChain'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.llm.LLMChain'> for base <class 'langchain.chains.base.Chain'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.llm.LLMChain'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict[str, Any], dict[str, Any]]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain.chains.llm.LLMChain'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain.chains.llm.LLMChain'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.prompt.PromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.string.StringPromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.prompts.base.BasePromptTemplate'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict, PromptValue]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.prompts.prompt.PromptTemplate'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'> for base <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'> for base <class 'langchain.chains.combine_documents.base.BaseCombineDocumentsChain'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'> for base <class 'langchain.chains.base.Chain'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict[str, Any], dict[str, Any]]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain.chains.combine_documents.stuff.StuffDocumentsChain'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.vectorstores.base.VectorStoreRetriever'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting _get_relevant_documents\n",
      "\tinstrumenting get_relevant_documents\n",
      "\tinstrumenting aget_relevant_documents\n",
      "\tinstrumenting _aget_relevant_documents\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.retrievers.BaseRetriever'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting _get_relevant_documents\n",
      "\tinstrumenting get_relevant_documents\n",
      "\tinstrumenting aget_relevant_documents\n",
      "\tinstrumenting _aget_relevant_documents\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.runnables.base.RunnableSerializable[str, list[Document]]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "instrumenting <class 'langchain_core.vectorstores.base.VectorStoreRetriever'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "instrumenting <class 'langchain.chains.retrieval_qa.base.RetrievalQA'> for base <class 'langchain.chains.retrieval_qa.base.RetrievalQA'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.retrieval_qa.base.RetrievalQA'> for base <class 'langchain.chains.retrieval_qa.base.BaseRetrievalQA'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.retrieval_qa.base.RetrievalQA'> for base <class 'langchain.chains.base.Chain'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting run\n",
      "\tinstrumenting arun\n",
      "\tinstrumenting _call\n",
      "\tinstrumenting __call__\n",
      "\tinstrumenting _acall\n",
      "\tinstrumenting acall\n",
      "instrumenting <class 'langchain.chains.retrieval_qa.base.RetrievalQA'> for base <class 'langchain_core.runnables.base.RunnableSerializable[dict[str, Any], dict[str, Any]]'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain.chains.retrieval_qa.base.RetrievalQA'> for base <class 'langchain_core.runnables.base.RunnableSerializable'>\n",
      "\tinstrumenting invoke\n",
      "\tinstrumenting ainvoke\n",
      "\tinstrumenting stream\n",
      "\tinstrumenting astream\n",
      "\tinstrumenting __call__\n",
      "instrumenting <class 'langchain.chains.retrieval_qa.base.RetrievalQA'> for base <class 'langchain_core.load.serializable.Serializable'>\n",
      "\tinstrumenting __call__\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcap/app/.venv/lib/python3.13/site-packages/urllib3/connectionpool.py:1097: InsecureRequestWarning: Unverified HTTPS request is being made to host 'genai-proxy.sys.tas-ndc.kuhn-labs.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#tls-warnings\n",
      "  warnings.warn(\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=3.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=2.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=1.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Context relevance score: {'query': 'Which aircraft equipment had hydraulic issues?', 'result': '<think>\\nOkay, the user is asking which aircraft equipment had hydraulic issues. Let me check the context provided.\\n\\nThe context mentions \"Landing Gear B: Hydraulic failure - Inspect hydraulic lines and refill fluid\" and \"Hydraulics / Actuation: Hydraulic system controlling flaps and landing gear.\" So, the hydraulic system is related to both the landing gear and the flaps. The specific equipment mentioned here are the landing gear and the flaps, as they are controlled by the hydraulic system. The problem stated is a hydraulic failure in the landing gear, so the answer should be the landing gear. But also, since the hydraulic system controls both flaps and landing gear, maybe the answer includes both? Wait, the user is asking which equipment had hydraulic issues. The context says \"Hydraulic failure\" under Landing Gear B, so the specific equipment is the landing gear. The hydraulics/actuation system is the system that controls flaps and landing gear, but the actual equipment with the issue is the landing gear. So the answer is the landing gear. But I need to make sure. The user might be looking for the landing gear. Also, the engine is mentioned as a high-bypass turbofan, but that\\'s unrelated. So the answer is the landing gear.\\n</think>\\n\\nThe aircraft equipment that had hydraulic issues was the **landing gear**. Specifically, the hydraulic system controlling the landing gear experienced a failure, requiring inspection of hydraulic lines and refilling fluid.'}\n",
      "                                      app_id                      app_name  \\\n",
      "0  app_hash_6e03a320e07ec0cec02056cc7d73786e  Aircraft Maintenance Chatbot   \n",
      "1  app_hash_6e03a320e07ec0cec02056cc7d73786e  Aircraft Maintenance Chatbot   \n",
      "2  app_hash_6e03a320e07ec0cec02056cc7d73786e  Aircraft Maintenance Chatbot   \n",
      "3  app_hash_6e03a320e07ec0cec02056cc7d73786e  Aircraft Maintenance Chatbot   \n",
      "4  app_hash_6e03a320e07ec0cec02056cc7d73786e  Aircraft Maintenance Chatbot   \n",
      "\n",
      "  app_version                                           app_json  type  \\\n",
      "0          v1  {'app_name': 'Aircraft Maintenance Chatbot', '...  SPAN   \n",
      "1          v1  {'app_name': 'Aircraft Maintenance Chatbot', '...  SPAN   \n",
      "2          v1  {'app_name': 'Aircraft Maintenance Chatbot', '...  SPAN   \n",
      "3          v1  {'app_name': 'Aircraft Maintenance Chatbot', '...  SPAN   \n",
      "4          v1  {'app_name': 'Aircraft Maintenance Chatbot', '...  SPAN   \n",
      "\n",
      "                              record_id input_id  \\\n",
      "0  e14ed295-0719-4839-9509-f4423086b5b5            \n",
      "1  659546c2-9ab0-4ffd-ba76-f2270454df0f            \n",
      "2  cb7c9312-37c7-47ee-8e13-66a9fa042059            \n",
      "3  b238a61d-57e8-40e1-8621-1b1c926f060a            \n",
      "4  8b7cf8b1-6db1-427a-b88c-a493912fd10b            \n",
      "\n",
      "                                            input  \\\n",
      "0  Which aircraft equipment had hydraulic issues?   \n",
      "1  Which aircraft equipment had hydraulic issues?   \n",
      "2  Which aircraft equipment had hydraulic issues?   \n",
      "3  Which aircraft equipment had hydraulic issues?   \n",
      "4  Which aircraft equipment had hydraulic issues?   \n",
      "\n",
      "                                              output tags  ...  \\\n",
      "0  <think>\\nOkay, let me try to figure out the an...       ...   \n",
      "1                                                          ...   \n",
      "2  <think>\\nOkay, the user is asking which aircra...       ...   \n",
      "3  <think>\\nOkay, let's see. The user is asking w...       ...   \n",
      "4  <think>\\nOkay, let's see. The user is asking w...       ...   \n",
      "\n",
      "                          ts    latency total_tokens total_cost  \\\n",
      "0 2025-09-05 21:26:35.167485  15.704959            0        0.0   \n",
      "1 2025-09-05 21:27:45.984392   2.291659            0        0.0   \n",
      "2 2025-09-05 21:31:14.859841  18.458154            0        0.0   \n",
      "3 2025-09-05 21:52:42.170558  19.836960            0        0.0   \n",
      "4 2025-09-08 17:32:12.265929   2.040539            0        0.0   \n",
      "\n",
      "   cost_currency  num_events  context_relevance  \\\n",
      "0            USD          17               None   \n",
      "1            USD          15               None   \n",
      "2            USD          13               None   \n",
      "3            USD          11               None   \n",
      "4            USD           9               None   \n",
      "\n",
      "                             context_relevance_calls  \\\n",
      "0  [{'span_type': 'eval', 'args': {'question': 'W...   \n",
      "1  [{'span_type': 'eval', 'args': {'question': 'W...   \n",
      "2  [{'span_type': 'eval', 'args': {'question': 'W...   \n",
      "3  [{'span_type': 'eval', 'args': {'question': 'W...   \n",
      "4  [{'span_type': 'eval', 'args': {'question': 'W...   \n",
      "\n",
      "   context_relevance feedback cost in USD context_relevance direction  \n",
      "0                                     0.0                        True  \n",
      "1                                     0.0                        True  \n",
      "2                                     0.0                        True  \n",
      "3                                     0.0                        True  \n",
      "4                                     0.0                        True  \n",
      "\n",
      "[5 rows x 23 columns]\n",
      "Starting dashboard ...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89fd3573cdb41eba542da4114192ee8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Accordion(children=(VBox(children=(VBox(children=(Label(value='STDOUT'), Output())), VBox(children=(Label(valu…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=0.\n",
      "feedback_name=context_relevance, record=659546c2-9ab0-4ffd-ba76-f2270454df0f, span_group=None had an error during computation:\n",
      "Endpoint OpenAIEndpoint request failed 4 time(s): \n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=3.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=2.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=1.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=0.\n",
      "feedback_name=context_relevance, record=8b7cf8b1-6db1-427a-b88c-a493912fd10b, span_group=None had an error during computation:\n",
      "Endpoint OpenAIEndpoint request failed 4 time(s): \n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=3.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=2.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=1.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=0.\n",
      "feedback_name=context_relevance, record=b238a61d-57e8-40e1-8621-1b1c926f060a, span_group=None had an error during computation:\n",
      "Endpoint OpenAIEndpoint request failed 4 time(s): \n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=3.\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Dashboard failed to start in time. Please inspect dashboard logs for additional information.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 72\u001b[39m\n\u001b[32m     70\u001b[39m tru.get_leaderboard(app_ids=[])\n\u001b[32m     71\u001b[39m \u001b[38;5;66;03m# Launch dashboard\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m \u001b[43mtru\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_dashboard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m        \u001b[49m\n\u001b[32m     73\u001b[39m \u001b[43m    \u001b[49m\u001b[43maddress\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43m0.0.0.0\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# bind to all network interfaces for external access\u001b[39;49;00m\n\u001b[32m     74\u001b[39m \u001b[43m    \u001b[49m\u001b[43mport\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m7861\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m       \u001b[49m\u001b[38;5;66;43;03m# port to run the dashboard\u001b[39;49;00m\n\u001b[32m     75\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/trulens/core/utils/deprecation.py:216\u001b[39m, in \u001b[36mmethod_renamed.<locals>.wrapper.<locals>._renamedmethod\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    213\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    214\u001b[39m     warned = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m216\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/trulens/core/session.py:496\u001b[39m, in \u001b[36mTruSession.run_dashboard\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m    488\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    489\u001b[39m \u001b[33;03m!!! warning \"Deprecated\"\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    Use\u001b[39;00m\n\u001b[32m    491\u001b[39m \u001b[33;03m    [trulens.dashboard.run.run_dashboard][trulens.dashboard.run.run_dashboard]\u001b[39;00m\n\u001b[32m    492\u001b[39m \u001b[33;03m    instead.\u001b[39;00m\n\u001b[32m    493\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    494\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtrulens\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mdashboard\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mrun\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m run_dashboard\n\u001b[32m--> \u001b[39m\u001b[32m496\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mrun_dashboard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msession\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/.venv/lib/python3.13/site-packages/trulens/dashboard/run.py:291\u001b[39m, in \u001b[36mrun_dashboard\u001b[39m\u001b[34m(session, port, address, force, sis_compatibility_mode, _dev, _watch_changes)\u001b[39m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m started.wait(timeout=wait_period):\n\u001b[32m    290\u001b[39m     session._dashboard_proc = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m291\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m    292\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mDashboard failed to start in time. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    293\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mPlease inspect dashboard logs for additional information.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    294\u001b[39m     )\n\u001b[32m    296\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m proc\n",
      "\u001b[31mRuntimeError\u001b[39m: Dashboard failed to start in time. Please inspect dashboard logs for additional information."
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=2.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=1.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=0.\n",
      "feedback_name=context_relevance, record=cb7c9312-37c7-47ee-8e13-66a9fa042059, span_group=None had an error during computation:\n",
      "Endpoint OpenAIEndpoint request failed 4 time(s): \n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=3.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=2.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=1.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=0.\n",
      "feedback_name=context_relevance, record=e14ed295-0719-4839-9509-f4423086b5b5, span_group=None had an error during computation:\n",
      "Endpoint OpenAIEndpoint request failed 4 time(s): \n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=3.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=2.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=1.\n",
      "OpenAIEndpoint request failed <class 'openai.NotFoundError'>=Error code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}. Retries remaining=0.\n",
      "feedback_name=context_relevance, record=5233db4c-e53c-4114-9d9c-b80dc21b1de7, span_group=None had an error during computation:\n",
      "Endpoint OpenAIEndpoint request failed 4 time(s): \n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n",
      "\tError code: 404 - {'type': 'about:blank', 'title': 'Not Found', 'status': 404, 'detail': 'No static resource prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses.', 'instance': '/prod-chat-tools-qwen3-ultra-48b5cd6/openai/v1/responses'}\n"
     ]
    }
   ],
   "source": [
    "# uv add trulens-core trulens-apps-langchain trulens-providers-openai\n",
    "import numpy as np\n",
    "from trulens.apps.langchain import TruChain\n",
    "from trulens.core import Feedback, TruSession, Tru\n",
    "from trulens.providers.openai import OpenAI\n",
    "\n",
    "import os\n",
    "import httpx\n",
    "\n",
    "from trulens.providers.openai.endpoint import OpenAIEndpoint\n",
    "\n",
    "vcapservices = os.getenv('VCAP_SERVICES')\n",
    "services = json.loads(vcapservices)\n",
    "\n",
    "def is_chatservice(service):\n",
    "    return service[\"name\"] == \"gen-ai-qwen3-ultra\"\n",
    "\n",
    "chat_services = filter(is_chatservice, services[\"genai\"])\n",
    "chat_credentials = list(chat_services)[0][\"credentials\"]\n",
    "\n",
    "# Specify your local endpoint URL and API key\n",
    "custom_api_base = chat_credentials[\"api_base\"] + \"/v1\"\n",
    "custom_api_key = chat_credentials[\"api_key\"]\n",
    "custom_model_name = chat_credentials[\"model_name\"]\n",
    "\n",
    "# Create a custom HTTP client with SSL verification disabled\n",
    "custom_http_client = httpx.Client(verify=False)\n",
    "\n",
    "\n",
    "# Create the TruLens OpenAI provider with the custom endpoint\n",
    "provider = CustomOpenAI(\n",
    "    model_engine=custom_model_name,\n",
    "    api_key=custom_api_key,\n",
    "    api_base=custom_api_base,\n",
    "    verify=False\n",
    ")\n",
    "# Initialize session\n",
    "tru = Tru()\n",
    "\n",
    "\n",
    "context = TruChain.select_context(qa)\n",
    "\n",
    "f_context_relevance = (\n",
    "    Feedback(provider.context_relevance)\n",
    "    .on_input()\n",
    "    .on_context(collect_list=False)\n",
    "    .aggregate(np.mean)\n",
    ")\n",
    "\n",
    "# Wrap and evaluate\n",
    "tru_recorder = TruChain(\n",
    "    qa,\n",
    "    app_name=\"Aircraft Maintenance Chatbot\",\n",
    "    app_id=\"app_aircraft_maintenance\",\n",
    "    app_version=\"v1\",\n",
    "    feedbacks=[f_context_relevance],\n",
    ")\n",
    "\n",
    "\n",
    "# Example query\n",
    "query = \"Which aircraft equipment had hydraulic issues?\"\n",
    "\n",
    "with tru_recorder as recording:\n",
    "    response = qa(query)\n",
    "    print(\"Context relevance score:\", response)\n",
    "    \n",
    "records, feedback = tru.get_records_and_feedback(app_ids=[])\n",
    "print(records.head())\n",
    "\n",
    "tru.get_leaderboard(app_ids=[])\n",
    "# Launch dashboard\n",
    "tru.run_dashboard(        \n",
    "    address=\"0.0.0.0\",  # bind to all network interfaces for external access\n",
    "    port=7861,       # port to run the dashboard\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2175c84a-e02b-420d-930b-26be886a8c81",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
