{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01e741c4-bf73-4f95-8265-8e85ecf75e1f",
   "metadata": {},
   "source": [
    "## Langgraph Essay Writer Agent \n",
    "\n",
    "You are an expert writer tasked with writing a high level outline of an essay.  Write such an outline for user provided topic.  Give an outline of the essay along with notes and instructions for the sections\n",
    "\n",
    "##### Note: Go to JupyterLab terminal and execute following command before getting started\n",
    "<pre>\n",
    "    uv add langgraph\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "316b0f92-e9cd-49d2-85fe-08a803e29513",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'planner': {'plan': '### **Essay Outline: Understanding the Difference Between LangChain and LangSmith**\\n\\n---\\n\\n#### **I. Introduction**\\n- **Purpose**: Introduce the growing importance of large language models (LLMs) in AI development.\\n- **Thesis Statement**: While both LangChain and LangSmith are tools in the LangChain ecosystem, they serve distinct purposes: LangChain focuses on **application development** using LLMs, while LangSmith centers on **model training, testing, and deployment**.\\n- **Notes**: Highlight the need for clarity in understanding these tools, especially for developers and AI practitioners.\\n\\n---\\n\\n#### **II. Defining the Tools**\\n**A. LangChain**  \\n- **Definition**: A framework for building applications with LLMs, emphasizing **data integration**, **chainable workflows**, and **customizable logic**.  \\n- **Key Features**:  \\n  - **Chains**: Enables sequential processing of data (e.g., combining multiple models).  \\n  - **Prompts**: Templates for generating text from LLMs.  \\n  - **LLM Abstraction**: Supports multiple models (e.g., GPT-3, Llama).  \\n- **Use Cases**: Building chatbots, data pipelines, or analytics tools.  \\n\\n**B. LangSmith**  \\n- **Definition**: A platform for **training, testing, and deploying LLMs**, focusing on **model development**.  \\n- **Key Features**:  \\n  - **Model Training**: Tools for fine-tuning models on specific datasets.  \\n  - **Evaluation**: Metrics to assess model performance.  \\n  - **Deployment**: Simplifies integrating models into production.  \\n- **Use Cases**: Refining a model for a niche task (e.g., medical diagnostics) or optimizing a modelâ€™s accuracy.  \\n\\n**Notes**: Emphasize that LangSmith is part of the broader LangChain ecosystem but focuses on the **model lifecycle**, while LangChain is about **application architecture**.\\n\\n---\\n\\n#### **III. Key Differences**\\n**A. Focus Area**  \\n- **LangChain**: Application development (e.g., building a customer support chatbot).  \\n- **LangSmith**: Model training and optimization (e.g., improving a modelâ€™s accuracy for a specific task).  \\n\\n**B. Target Audience**  \\n- **LangChain**: Developers and engineers creating LLM-powered applications.  \\n- **LangSmith**: Data scientists and ML engineers refining models.  \\n\\n**C. Core Functionality**  \\n- **LangChain**: Manages **data flow** and **logic chains**.  \\n- **LangSmith**: Manages **model training**, **evaluation**, and **deployment**.  \\n\\n**D. Integration**  \\n- **LangChain** integrates with **LangSmith** for end-to-end workflows (e.g., training a model in LangSmith and deploying it via LangChain).  \\n\\n**Notes**: Use a table or bullet points to summarize the differences clearly.\\n\\n---\\n\\n#### **IV. Use Cases and Examples**\\n- **LangChain Example**:  \\n  - A company builds a customer support chatbot using LangChainâ€™s chains and prompts to process user queries.  \\n- **LangSmith Example**:  \\n  - A team trains a custom LLM on medical data to diagnose patient symptoms, using LangSmithâ€™s tools for evaluation and deployment.  \\n\\n**Notes**: Highlight scenarios where one tool is preferable (e.g., LangSmith for model development, LangChain for application logic).\\n\\n---\\n\\n#### **V. Conclusion**\\n- **Summary**: Reiterate that LangChain is for **application development**, while LangSmith is for **model training and deployment**.  \\n- **Importance**: Emphasize the complementary roles of both tools in the AI development lifecycle.  \\n- **Final Thought**: Encourage readers to choose the tool based on their specific needs (application vs. model development).\\n\\n---\\n\\n### **Instructions for Writing the Essay**\\n1. **Introduction**: Start with a hook about AIâ€™s role in modern tech, then introduce the two tools.  \\n2. **Definitions**: Clearly explain each toolâ€™s purpose, features, and target audience.  \\n3. **Key Differences**: Use subheadings (e.g., Focus Area, Target Audience) and include examples.  \\n4. **Use Cases**: Provide real-world scenarios to illustrate practical applications.  \\n5. **Conclusion**: Tie together the importance of both tools and their synergy.  \\n6. **Tone**: Maintain a professional yet accessible tone, avoiding overly technical jargon.  \\n\\nLet me know if you need a full draft or further clarification!'}}\n",
      "{'research_plan': {'content': ['In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'Jul 12, 2025 Â· Know the basic difference between Langchain tools LangChain : Your Toolkit for Building Complex LLM Applications What it does : LangChain is a foundational framework designed for developing applications powered by LLMs.', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'Compare LangChain, LangSmith, and Orq.ai to discover the best LLM development tools for building, deploying, and optimizing scalable AI applications. As a **comprehensive platform** for LLM product development, LangChain equips software teams with the tools needed to build, test, and deploy LLM-powered solutions at scale. While LangChain focuses on the flexibility and modularity required for building LLM-powered applications, LangSmith steps in to offer essential tools for deployment, monitoring, and optimization throughout the production process. Orq.ai offers several advantages over **LangChain** and **LangSmith**, providing a more integrated and efficient solution for LLM development. By offering an integrated solution that supports the entire LLM development lifecycle, Orq.ai enables teams to seamlessly build, deploy, and optimize LLM applications at scale, without needing to juggle multiple specialized tools.']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nThe rise of large language models (LLMs) has revolutionized artificial intelligence, enabling developers to create sophisticated applications that understand and generate human-like text. Amid this AI boom, tools like LangChain and LangSmith have emerged, each tailored to distinct aspects of AI development. While both are part of the broader LangChain ecosystem, their purposes diverge sharply: LangChain is a framework designed for building LLM-powered applications, whereas LangSmith is a platform focused on training, testing, and deploying models. Understanding this distinction is critical for developers and AI practitioners, as it clarifies when to prioritize application logic versus model refinement.  \\n\\nLangChain centers on *application development*, offering a modular framework to integrate LLMs into workflows. It emphasizes features like *chains*, which allow sequential processing of data (e.g., combining multiple models or steps), and *prompts*, which act as templates for generating text. Its abstraction layer supports diverse models, such as GPT-3 or Llama, making it ideal for tasks like chatbots, data pipelines, or analytics tools. In contrast, LangSmith is engineered for *model development*, providing tools to train, evaluate, and deploy LLMs. It includes functionalities like fine-tuning models on specialized datasets, measuring performance through metrics, and streamlining deployment. While LangChain manages how LLMs interact with users or systems, LangSmith focuses on optimizing the models themselves.  \\n\\nThe key differences between the two tools lie in their focus, audience, and functionality. LangChain caters to developers seeking to build applications, emphasizing data flow and customizable logic. LangSmith, on the other hand, targets data scientists and ML engineers refining models for specific tasks, such as medical diagnostics or customer service. Functionally, LangChain handles *application architecture*, while LangSmith manages *model lifecycle*â€”training, evaluation, and deployment. Notably, these tools complement each other: a team might use LangSmith to train a model and then integrate it into a LangChain-powered application for real-world use.  \\n\\nReal-world examples highlight their distinct roles. A company could use LangChain to develop a customer support chatbot, leveraging chains to process queries and prompts to generate responses. Meanwhile, a healthcare organization might employ LangSmith to fine-tune a model on patient data, ensuring accurate symptom analysis. In both cases, the choice of tool depends on the goal: LangChain for application logic and LangSmith for model optimization. These tools collectively bridge the gap between raw AI capabilities and practical implementation, empowering developers to tailor solutions to their needs.  \\n\\nIn conclusion, LangChain and LangSmith serve as twin pillars in the AI development ecosystem, each addressing a critical phase of the process. While LangChain enables the construction of LLM-driven applications, LangSmith ensures those models are accurate, efficient, and production-ready. Their synergy underscores the importance of choosing the right tool for the task at handâ€”whether the goal is to build an application or refine a model. As AI continues to evolve, these tools will remain essential for translating theoretical advancements into real-world impact.', 'revision_number': 2}}\n",
      "{'reflect': {'critique': '**Essay Critique and Recommendations**  \\n\\nYour essay provides a clear and structured overview of the differences between LangChain and LangSmith, effectively addressing their purposes, audiences, and use cases. Below are detailed recommendations to further strengthen your work:  \\n\\n---\\n\\n### **Strengths of the Essay**  \\n1. **Clear Organization**: The essay follows a logical flow, starting with an introduction, moving through technical distinctions, and concluding with real-world applications.  \\n2. **Relevant Examples**: The use of examples (e.g., customer support chatbots, healthcare applications) helps contextualize the toolsâ€™ roles.  \\n3. **Distinctive Focus**: You clearly separate LangChainâ€™s application-centric design from LangSmithâ€™s model-development focus, which is the essayâ€™s strongest asset.  \\n\\n---\\n\\n### **Areas for Improvement**  \\n\\n#### **1. Depth of Technical Analysis**  \\nWhile the essay explains the toolsâ€™ purposes, it could delve deeper into **how** they function technically. For instance:  \\n- **LangChain**: Expand on *chains* and *prompts* with concrete examples (e.g., a chain that integrates a question-answering model with a database lookup).  \\n- **LangSmith**: Clarify concepts like *fine-tuning* (e.g., how hyperparameters or datasets influence model performance) or *metrics* (e.g., BLEU scores for evaluation).  \\n\\n**Recommendation**: Add 1â€“2 paragraphs with technical details, such as:  \\n- A step-by-step explanation of a chain in LangChain (e.g., â€œA chain might first process user input through a prompt template, then pass the output to a language model, and finally format the responseâ€).  \\n- A description of LangSmithâ€™s *model lifecycle* (e.g., â€œLangSmith allows users to track training iterations, compare models using metrics like accuracy or latency, and deploy optimized versions to productionâ€).  \\n\\n---\\n\\n#### **2. Audience and Use Cases**  \\nThe essay mentions that LangChain targets developers and LangSmith targets data scientists, but it could benefit from **specific scenarios** to highlight these differences.  \\n\\n**Recommendation**:  \\n- **LangChain**: Describe a developerâ€™s workflow (e.g., building a chatbot with LangChainâ€™s chains to handle complex logic like user authentication or data retrieval).  \\n- **LangSmith**: Explain a data scientistâ€™s process (e.g., using LangSmith to fine-tune a medical diagnostic model on a dataset of patient symptoms and outcomes).  \\n- **Cross-Tool Use**: Include a case study (e.g., â€œA team uses LangSmith to train a sentiment analysis model, then integrates it into a LangChain application for real-time customer feedback analysisâ€).  \\n\\n---\\n\\n#### **3. Depth of Comparison**  \\nThe essay distinguishes between the tools but could strengthen its analysis by explicitly addressing **trade-offs** and **interdependencies**.  \\n\\n**Recommendation**:  \\n- **When to Use Each**: Add a section comparing scenarios (e.g., â€œUse LangChain for applications requiring custom workflows, and LangSmith for models needing iterative refinementâ€).  \\n- **Ecosystem Synergy**: Highlight how the tools complement each other (e.g., â€œLangSmithâ€™s model optimizations can be seamlessly integrated into LangChain applications, enabling scalable, production-ready solutionsâ€).  \\n\\n---\\n\\n#### **4. Style and Clarity**  \\nThe essay is well-written, but some sentences could be tightened for clarity.  \\n\\n**Example**:  \\n- Original: *â€œLangChain centers on *application development*, offering a modular framework to integrate LLMs into workflows.â€*  \\n- Suggestion: *â€œLangChain is designed for *application development*, providing a modular framework to integrate LLMs into workflows. Its key features include *chains* (for sequential processing) and *prompts* (for text generation).â€*  \\n\\n**Recommendation**:  \\n- Break long sentences into shorter ones for readability.  \\n- Use bullet points or numbered lists for technical features (e.g., â€œLangChainâ€™s key features include: 1. Chains for sequential processingâ€¦ 2. Prompts as text-generation templatesâ€¦â€).  \\n\\n---\\n\\n#### **5. Length and Depth**  \\nThe essay is concise but could benefit from **expanded analysis** of the toolsâ€™ implications for AI development.  \\n\\n**Recommendation**:  \\n- Add a paragraph on the **broader impact** of these tools:  \\n  - How do they democratize AI development?  \\n  - What challenges remain (e.g., scalability, cost of training models in LangSmith)?  \\n  - How do they align with industry trends (e.g., MLOps, low-code platforms)?  \\n\\n---\\n\\n### **Final Suggestions**  \\n1. **Add 1â€“2 paragraphs of technical depth** on how each tool operates (e.g., chains, prompts, fine-tuning).  \\n2. **Incorporate specific use cases** to clarify audience needs.  \\n3. **Enhance comparisons** by addressing trade-offs and synergies.  \\n4. **Tighten prose** for clarity and flow.  \\n\\nBy expanding on these elements, your essay will offer a more comprehensive and nuanced analysis of LangChain and LangSmith. Keep up the excellent work! ğŸš€'}}\n",
      "{'research_critique': {'content': ['In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'LangChain and LangSmith are two powerful tools developed by LangChain, a company focused on making it easier to build and deploy Large Language Model (LLM) applications. 1. Limited Scalability: LangChain is not designed for large-scale production environments, making it less suitable for complex, high-traffic applications. 1. Comprehensive Platform: LangSmith offers a unified platform for managing all aspects of LLM development, making it ideal for large-scale, production-ready applications. LangChain and LangSmith are two complementary tools that cater to different stages and requirements of LLM development. LangChain is ideal for early-stage prototyping and small-scale applications, while LangSmith is better suited for large-scale, production-ready applications that require advanced debugging, testing, and monitoring capabilities. ## Understanding LangChain Tools and Agents: A Guide to Building Smart AI Applications', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'Jul 12, 2025 Â· Know the basic difference between Langchain tools LangChain : Your Toolkit for Building Complex LLM Applications What it does : LangChain is a foundational framework designed for developing applications powered by LLMs.', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', 'Compare LangChain, LangSmith, and Orq.ai to discover the best LLM development tools for building, deploying, and optimizing scalable AI applications. As a **comprehensive platform** for LLM product development, LangChain equips software teams with the tools needed to build, test, and deploy LLM-powered solutions at scale. While LangChain focuses on the flexibility and modularity required for building LLM-powered applications, LangSmith steps in to offer essential tools for deployment, monitoring, and optimization throughout the production process. Orq.ai offers several advantages over **LangChain** and **LangSmith**, providing a more integrated and efficient solution for LLM development. By offering an integrated solution that supports the entire LLM development lifecycle, Orq.ai enables teams to seamlessly build, deploy, and optimize LLM applications at scale, without needing to juggle multiple specialized tools.', 'This repository contains examples of using the LangChain framework to interact with Large Language Models (LLMs) for different prompt construction and execution techniques. Each script demonstrates a different approach for creating and using prompts with LLMs in Python, leveraging LangChain â€™s utilities for managing prompts , output parsing, and chain execution.', 'It simplifies creating complex workflows that leverage natural language understanding, chaining multiple tasks, and integrating external tools like APIs and databases. LangChain helps developers harness the power of LLMs by offering components that facilitate chaining prompts, integrating external tools, and maintaining context. Whether youâ€™re creating a chatbot, an intelligent assistant, or a text analysis tool, LangChain provides the building blocks to design complex workflows efficiently. from langchain.chains import SimpleSequentialChain chain1 = SimpleSequentialChain(llm=llm, prompt=template1) chain2 = SimpleSequentialChain(llm=llm, prompt=template2) output = final_chain.run(\"LangChain\") from langchain.agents import initialize_agent, Tool from langchain.tools import Tool from langchain.chains import ConversationChain from langchain.chains import RetrievalQA texts = [\"LangChain simplifies LLM workflows.\", \"LangChain supports tools and memory.\"] By leveraging components like prompt templates, chains, agents, tools, and memory, you can create sophisticated workflows tailored to various use cases.', 'February 17, 2025 - LangChain is a powerful framework designed to simplify and enhance the integration of large language models (LLMs) into various applications . It provides a structured way to work with prompts and chains, making it easier to build complex workflows with LLMs.', 'LangChain is the open, composable framework that provides a standard interface for every model, tool, and database â€“ so you can build LLM apps that adapt as fast as the ecosystem evolves. ## Why use LangChain? How do I use LangChain with other products like LangSmith, LangGraph, or LangGraph Platform? LangChain provides a standard interface for connecting models, tools, and data, then integrates seamlessly with any of the Lang- family products. Use LangChain when you need fast integration and experimentation; use LangGraph when you need to build agents that can reliably handle complex tasks. Should I start with LangChain or LangGraph for building agents? Use **LangChain** for **composability and model flexibility**â€” great for quickly chaining LLMs with tools, retrievers, and external data sources.', 'LangChain ğŸ¦œï¸ğŸ”— ä¸­æ–‡ç½‘ï¼Œè·Ÿç€LangChainä¸€èµ·å­¦LLM/GPTå¼€å‘JS/TS Langchain JS/TS Langchain (opens in a new tab)Python Langchain Python Langchain (opens in a new tab)OpenAI ä¸­æ–‡æ–‡æ¡£ OpenAI ä¸­æ–‡æ–‡æ¡£ (opens in a new tab) GitHub (opens in a new tab) *   JS/TS Langchain (opens in a new tab) *   Python Langchain (opens in a new tab) *   OpenAI ä¸­æ–‡æ–‡æ¡£ (opens in a new tab) *   Pinecone ä¸­æ–‡æ–‡æ¡£ (opens in a new tab) *   Milvus ä¸­æ–‡æ–‡æ¡£ (opens in a new tab) LangSmith (opens in a new tab) å¸®åŠ©æ‚¨è·Ÿè¸ªå’Œè¯„ä¼°æ‚¨çš„è¯­è¨€æ¨¡å‹åº”ç”¨ç¨‹åºå’Œæ™ºèƒ½ä»£ç†ï¼Œä»¥å¸®åŠ©æ‚¨ä»åŸå‹è¿‡æ¸¡åˆ°ç”Ÿäº§ã€‚ æœ‰å…³æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚é˜…LangSmith æ–‡æ¡£ (opens in a new tab)ã€‚ è¦äº†è§£å¦‚ä½•é›†æˆ LangSmith åˆ°æ‚¨çš„å·¥ä½œæµç¨‹ä¸­ï¼Œä»¥åŠæ¼”ç¤ºç«¯åˆ°ç«¯ç¤ºä¾‹ï¼Œè¯·æŸ¥çœ‹LangSmith Cookbook (opens in a new tab)ã€‚å…¶ä¸­ä¸€äº›æŒ‡å—åŒ…æ‹¬ï¼š *   åœ¨ JS åº”ç”¨ç¨‹åºä¸­åˆ©ç”¨ç”¨æˆ·åé¦ˆ (é“¾æ¥ (opens in a new tab)) *   æ„å»ºè‡ªåŠ¨åŒ–åé¦ˆæµæ°´çº¿ (é“¾æ¥ (opens in a new tab)) *   å¦‚ä½•ä½¿ç”¨çœŸå®ä½¿ç”¨æ•°æ®å¯¹ LLM è¿›è¡Œå¾®è°ƒ (é“¾æ¥ (opens in a new tab))', '**LangChainã€LangGraph å’Œ LangSmith æ­£åœ¨å‘å±•å£®å¤§å¹¶æ‹›è˜å¤šä¸ªèŒä½ã€‚åŠ å…¥æˆ‘ä»¬çš„å›¢é˜Ÿï¼** # LangSmith å…¥é—¨ **LangSmith** æ˜¯ä¸€ä¸ªç”¨äºæ„å»ºç”Ÿäº§çº§ LLM åº”ç”¨ç¨‹åºçš„å¹³å°ã€‚å®ƒèƒ½è®©æ‚¨å¯†åˆ‡ç›‘æ§å’Œè¯„ä¼°æ‚¨çš„åº”ç”¨ç¨‹åºï¼Œä»è€Œå¸®åŠ©æ‚¨å¿«é€Ÿã€è‡ªä¿¡åœ°äº¤ä»˜äº§å“ã€‚ ### å¯è§‚æµ‹æ€§ åœ¨ LangSmith ä¸­åˆ†æè·Ÿè¸ªï¼Œå¹¶åŸºäºæ­¤é…ç½®æŒ‡æ ‡ã€ä»ªè¡¨æ¿å’Œè­¦æŠ¥ã€‚ ### è¯„ä¼° æ ¹æ®ç”Ÿäº§æµé‡è¯„ä¼°æ‚¨çš„åº”ç”¨ç¨‹åº â€” è¡¡é‡åº”ç”¨ç¨‹åºæ€§èƒ½å¹¶è·å–æ‚¨æ•°æ®çš„äººå·¥åé¦ˆã€‚ ### æç¤ºå·¥ç¨‹ è¿­ä»£æç¤ºï¼Œå¹¶æ”¯æŒè‡ªåŠ¨ç‰ˆæœ¬æ§åˆ¶å’Œåä½œåŠŸèƒ½ã€‚ LangSmith + LangChain å¼€æºç‰ˆ LangSmith æ˜¯ä¸æ¡†æ¶æ— å…³çš„ â€” å®ƒå¯ä»¥ä¸ LangChain çš„å¼€æºæ¡†æ¶ `langchain` å’Œ `langgraph` ä¸€èµ·ä½¿ç”¨ï¼Œä¹Ÿå¯ä»¥å•ç‹¬ä½¿ç”¨ã€‚ å¦‚æœæ‚¨æ­£åœ¨ä½¿ç”¨å…¶ä¸­ä»»ä½•ä¸€ä¸ªï¼Œæ‚¨å¯ä»¥é€šè¿‡ä¸€ä¸ªç¯å¢ƒå˜é‡å¯ç”¨ LangSmith è·Ÿè¸ªã€‚æ›´å¤šä¿¡æ¯è¯·å‚é˜…ä½¿ç”¨ LangChain è®¾ç½® LangSmith æˆ– ä½¿ç”¨ LangGraph è®¾ç½® LangSmith çš„æ“ä½œæŒ‡å—ã€‚ ## å¯è§‚æµ‹æ€§\\u200b å¯è§‚æµ‹æ€§å¯¹äºä»»ä½•è½¯ä»¶åº”ç”¨ç¨‹åºéƒ½è‡³å…³é‡è¦ï¼Œå°¤å…¶æ˜¯å¯¹äº LLM åº”ç”¨ç¨‹åºã€‚LLM æœ¬èº«æ˜¯éç¡®å®šæ€§çš„ï¼Œè¿™æ„å‘³ç€å®ƒä»¬å¯èƒ½ä¼šäº§ç”Ÿæ„æƒ³ä¸åˆ°çš„ç»“æœã€‚è¿™ä½¿å¾—å®ƒä»¬æ¯”ä¸€èˆ¬çš„åº”ç”¨ç¨‹åºæ›´éš¾è°ƒè¯•ã€‚ è¿™æ­£æ˜¯ LangSmith å¯ä»¥æä¾›å¸®åŠ©çš„åœ°æ–¹ï¼LangSmith æ‹¥æœ‰ LLM åŸç”Ÿçš„å¯è§‚æµ‹æ€§ï¼Œè®©æ‚¨èƒ½å¤Ÿä»åº”ç”¨ç¨‹åºä¸­è·å¾—æœ‰æ„ä¹‰çš„æ´å¯Ÿã€‚LangSmith çš„å¯è§‚æµ‹æ€§åŠŸèƒ½æ¶µç›–äº†åº”ç”¨ç¨‹åºå¼€å‘çš„æ‰€æœ‰é˜¶æ®µâ€”â€”ä»åŸå‹è®¾è®¡ã€å†…æµ‹åˆ°ç”Ÿäº§ã€‚ * é¦–å…ˆï¼Œä¸ºæ‚¨çš„åº”ç”¨ç¨‹åºæ·»åŠ è·Ÿè¸ªã€‚ * åˆ›å»ºä»ªè¡¨æ¿ä»¥æŸ¥çœ‹ RPSã€é”™è¯¯ç‡å’Œæˆæœ¬ç­‰å…³é”®æŒ‡æ ‡ã€‚ ## è¯„ä¼°\\u200b AI åº”ç”¨ç¨‹åºçš„è´¨é‡å’Œå¼€å‘é€Ÿåº¦å–å†³äºé«˜è´¨é‡çš„è¯„ä¼°æ•°æ®é›†å’ŒæŒ‡æ ‡ï¼Œä»¥ä¾¿å¯¹åº”ç”¨ç¨‹åºè¿›è¡Œæµ‹è¯•å’Œä¼˜åŒ–ã€‚LangSmith SDK å’Œ UI ä½¿æ„å»ºå’Œè¿è¡Œé«˜è´¨é‡è¯„ä¼°å˜å¾—å®¹æ˜“ã€‚ * é¦–å…ˆï¼Œåˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªè¯„ä¼°ã€‚ * ä½¿ç”¨æˆ‘ä»¬ç°æˆçš„è¯„ä¼°å™¨ä½œä¸ºèµ·ç‚¹ï¼Œå¿«é€Ÿè¯„ä¼°æ‚¨çš„åº”ç”¨ç¨‹åºæ€§èƒ½ã€‚ * åœ¨ LangSmith UI ä¸­åˆ†æè¯„ä¼°ç»“æœï¼Œå¹¶æ¯”è¾ƒéšæ—¶é—´å˜åŒ–çš„ç»“æœã€‚ * è½»æ¾æ”¶é›†å…³äºæ‚¨æ•°æ®çš„äººå·¥åé¦ˆï¼Œä»¥æ”¹è¿›æ‚¨çš„åº”ç”¨ç¨‹åºã€‚ ## æç¤ºå·¥ç¨‹\\u200b ä¼ ç»Ÿè½¯ä»¶åº”ç”¨ç¨‹åºé€šè¿‡ç¼–å†™ä»£ç æ¥æ„å»ºï¼Œè€Œ AI åº”ç”¨ç¨‹åºåˆ™æ¶‰åŠç¼–å†™æç¤ºæ¥æŒ‡å¯¼ LLM æ‰§è¡Œæ“ä½œã€‚LangSmith æä¾›äº†ä¸€å¥—å·¥å…·ï¼Œæ—¨åœ¨æ”¯æŒå’Œä¿ƒè¿›æç¤ºå·¥ç¨‹ï¼Œå¸®åŠ©æ‚¨ä¸ºåº”ç”¨ç¨‹åºæ‰¾åˆ°å®Œç¾çš„æç¤ºã€‚ * é¦–å…ˆï¼Œåˆ›å»ºæ‚¨çš„ç¬¬ä¸€ä¸ªæç¤ºã€‚ * ä½¿ç”¨æ“åœºï¼ˆPlaygroundï¼‰è¿­ä»£æ¨¡å‹å’Œæç¤ºã€‚ * åœ¨æ‚¨çš„åº”ç”¨ç¨‹åºä¸­ä»¥ç¼–ç¨‹æ–¹å¼ç®¡ç†æç¤ºã€‚ #### æ­¤é¡µé¢æœ‰å¸®åŠ©å—ï¼Ÿ #### æ‚¨å¯ä»¥ç•™ä¸‹è¯¦ç»†åé¦ˆ åœ¨ GitHub ä¸Š. * å¯è§‚æµ‹æ€§ * è¯„ä¼° * æç¤ºå·¥ç¨‹', 'June 25, 2025 - LLM Evaluation : Practical insights on metrics , frameworks, tools, challenges, and best practices for robust AI performance.', '* Understand the importance of observability in LLM applications and how to implement it using LangSmith for real-time monitoring and debugging. LangSmith is a state-of-the-art testing framework designed for the evaluation of language models and AI applications, with a particular emphasis on creating production-grade LLM applications. Setting LANGCHAIN\\\\_TRACING\\\\_V2 to true enables tracing (logging), which is essential for debugging LLMs. Once you run the create\\\\_project command successfully, you will see the project listed in the Projects section of the LangSmith web UI. This code snippet demonstrates AI-assisted feedback, where an LLM (GPT-3.5-turbo) scores each runâ€™s input based on several metrics (relevance, difficulty, verbosity, and specificity). By utilizing LangSmithâ€™s monitoring, evaluation, debugging, testing, tracing, and observability functions, developers and businesses can significantly improve their modelâ€™s performance and reliability.', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.', '* LangChain: The Foundation of AI Application Development The LangChain ecosystem has grown into a powerful toolkit for building, managing, and scaling AI applications. ## LangChain: The Foundation of AI Application Development LangGraph is the next step in building powerful AI workflows. With its drag-and-drop interface, you can design advanced AI workflows without writing much code, perfect for both developers and non-technical users. LangSmith is the observability and evaluation platform for AI applications--whether built with LangChain or not. It helps teams monitor, debug, test, and optimize LLM-based systems with deep visibility into how your AI behaves in real-world scenarios. At Techvoot, we help startups and enterprises design, develop, and optimize custom LLM solutions using LangChain and the broader AI ecosystem.', '* LangChain: The Foundation of AI Application Development The LangChain ecosystem has grown into a powerful toolkit for building, managing, and scaling AI applications. ## LangChain: The Foundation of AI Application Development LangGraph is the next step in building powerful AI workflows. With its drag-and-drop interface, you can design advanced AI workflows without writing much code, perfect for both developers and non-technical users. LangSmith is the observability and evaluation platform for AI applications--whether built with LangChain or not. It helps teams monitor, debug, test, and optimize LLM-based systems with deep visibility into how your AI behaves in real-world scenarios. At Techvoot, we help startups and enterprises design, develop, and optimize custom LLM solutions using LangChain and the broader AI ecosystem.', 'In LLM application development, LangChain and LangSmith have become central tools for building and managing large language model-powered solutions. This article compares LangChain and LangSmith, focusing on their core features, integration options, and value for developers in the LLM application space. LangChain is an open-source framework that helps developers create LLM applications efficiently. LangSmith provides tools to debug, monitor, and improve LLM-powered agents, and offers a managed cloud service with a web UI. | LLM Evaluation | Minimal built-in support; developers typically create custom logic or use external tools. | Key Features | Modular chains and sequences, prompt templates, agent framework, data connectors, wide model support, community integrations. LangChain provides building blocks for LLM applications, while LangSmith offers observability and evaluation.']}}\n",
      "{'generate': {'draft': '**The Difference Between LangChain and LangSmith**  \\n\\nThe rapid evolution of large language models (LLMs) has transformed artificial intelligence, enabling innovations across industries. As developers and researchers seek tools to harness these models, distinctions between platforms like **LangChain** and **LangSmith** become critical. While both tools belong to the broader LangChain ecosystem, their purposes diverge sharply: **LangChain** is designed for **building applications** with LLMs, whereas **LangSmith** focuses on **training, testing, and deploying models**. Understanding this distinction is vital for professionals aiming to optimize AI workflows.  \\n\\n**LangChain** serves as a framework for developers seeking to integrate LLMs into real-world applications. It emphasizes **data flow** and **customizable logic** through features like *chains* (which allow sequential processing of tasks), *prompts* (templates for generating text), and *LLM abstraction* (supporting multiple models). For example, a developer might use LangChain to create a chatbot that processes user inputs, consults a knowledge base, and generates responses. Its strength lies in enabling complex applications, such as analytics tools or customer service platforms, by abstracting the intricacies of model interactions. In contrast, **LangSmith** is tailored for **model development**, offering tools to train, evaluate, and deploy LLMs. It provides functionalities like *fine-tuning models on specific datasets*, *evaluating performance metrics*, and *streamlining deployment*. A medical AI team, for instance, might use LangSmith to refine a modelâ€™s accuracy in diagnosing patient symptoms.  \\n\\nThe core difference between the two lies in their **focus areas** and **audiences**. **LangChain** caters to **developers and engineers** who prioritize application logic, such as integrating LLMs into workflows or optimizing user interactions. **LangSmith**, on the other hand, targets **data scientists and ML engineers** focused on model refinement. While LangChain manages data processing and logic chains, LangSmith handles the *model lifecycle*, from training to production. Their complementary roles are evident in workflows: a team might train a model in LangSmith and then deploy it via LangChain, creating a seamless pipeline.  \\n\\nReal-world applications highlight their unique strengths. **LangChain** excels in scenarios requiring *application architecture*, such as building a virtual assistant that aggregates data from multiple sources. **LangSmith** shines in *model-centric tasks*, like enhancing a fraud detection system by fine-tuning a model on transactional data. For instance, a startup developing a legal research tool might use LangChain to structure queries and analyze case law, while a healthcare organization could rely on LangSmith to train a model for interpreting medical scans. These examples underscore how each tool addresses distinct challenges.  \\n\\nIn conclusion, **LangChain** and **LangSmith** are not interchangeable but complementary. **LangChain** is the backbone of application development, offering flexibility for integrating LLMs into complex systems. **LangSmith** is the cornerstone of model development, empowering users to train and optimize AI models. Together, they form a cohesive ecosystem, enabling professionals to innovate at both the application and model levels. By choosing the right toolâ€”**application development** versus **model training**â€”developers can unlock the full potential of LLMs, driving progress in AI.', 'revision_number': 3}}\n"
     ]
    }
   ],
   "source": [
    "#set up your-tavily-key\n",
    "from langgraph.graph import StateGraph, END\n",
    "from typing import TypedDict, Annotated, List, Optional\n",
    "import operator\n",
    "from langchain_core.messages import AnyMessage, SystemMessage, HumanMessage, ToolMessage\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "from openai import OpenAI\n",
    "import httpx\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.chains import LLMChain\n",
    "from langchain_openai import ChatOpenAI\n",
    "from langchain.agents import tool\n",
    "from langchain.agents import initialize_agent, AgentType, load_tools\n",
    "from langchain_core.tools import Tool\n",
    "from pydantic import BaseModel, ValidationError\n",
    "#from langchain_core.pydantic_v1 import BaseModel\n",
    "from langchain.tools import tool\n",
    "from langchain.chains.router import MultiPromptChain\n",
    "from langchain.chains.router.llm_router import LLMRouterChain,RouterOutputParser\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langgraph.checkpoint.sqlite import SqliteSaver\n",
    "from tavily import TavilyClient\n",
    "from langchain_community.tools.tavily_search import TavilySearchResults\n",
    "from IPython.display import Image\n",
    "import re\n",
    "import json\n",
    "from typing import Dict, Any\n",
    "from cfenv import AppEnv\n",
    "\n",
    "\n",
    "# connect to tavily search tool - use your tavily api key\n",
    "os.environ['TAVILY_API_KEY']=\"your_tavily_key\"\n",
    "tavily = TavilyClient(api_key=os.environ[\"TAVILY_API_KEY\"])\n",
    "#tool = TavilySearchResults(max_results=2)\n",
    "\n",
    "#define agent state\n",
    "class AgentState(TypedDict):\n",
    "    task: str\n",
    "    lnode: str\n",
    "    plan: str\n",
    "    draft: str\n",
    "    critique: str\n",
    "    content: List[str]\n",
    "    queries: List[str]\n",
    "    revision_number: int\n",
    "    max_revisions: int\n",
    "    count: Annotated[int, operator.add]\n",
    "\n",
    "\n",
    "#define and configure the model\n",
    "# Load CF environment\n",
    "env = AppEnv()\n",
    "# configure model\n",
    "httpx_client = httpx.Client(http2=True, verify=False, timeout=10.0)\n",
    "# Get bound service \"gen-ai-qwen3-ultra\"\n",
    "chat_service = env.get_service(name=\"gen-ai-qwen3-ultra\")\n",
    "chat_credentials = chat_service.credentials\n",
    "\n",
    "# Initialize LLM with credentials from cfenv\n",
    "model = ChatOpenAI(\n",
    "    temperature=0.9,\n",
    "    model=chat_credentials[\"model_name\"],\n",
    "    base_url=chat_credentials[\"api_base\"],\n",
    "    api_key=chat_credentials[\"api_key\"],\n",
    "    http_client=httpx_client\n",
    ")\n",
    "\n",
    "#define prompts\n",
    "PLAN_PROMPT = \"\"\"\n",
    "You are an expert writer tasked with writing a high level outline of an eassy. \\\n",
    "Write such an outline for the user provided topic. Give an outline of eassy along \\\n",
    "with any relevant notes or instructions for the sections.\n",
    "\"\"\"\n",
    "\n",
    "WRITER_PROMPT = \"\"\"\n",
    "You are an eassy assistant tasked with writing excellent 5-paragraph eassys. \\\n",
    "Generate the best eassy possible for the user's request and the initial outline. \\\n",
    "If the user provides critique, respond with a revised version of ypur previous attempts. \\\n",
    "\n",
    "--------\n",
    "\n",
    "{content}\"\"\"\n",
    "\n",
    "REFLECTION_PROMPT = \"\"\"\n",
    "You are a teacher grading an eassy submission. \\\n",
    "Generate critique and recommendations for the user's submission. \\\n",
    "Provide detailed recommendations, including requests for length, depth, style, etc.\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_PLAN_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when writing the following eassy. Generate a list of search queries that will gather \\\n",
    "any relevant information. Only generate 3 queries max\n",
    "\"\"\"\n",
    "\n",
    "RESEARCH_CRITIQUE_PROMPT = \"\"\"\n",
    "You are a researcher charged with providing information that can \\\n",
    "be used when making any requested revisions (as oulined below). \\\n",
    "Generate a list of search queries that will gather any relevant information. \\\n",
    "Only generate 3 queries max.\n",
    "\"\"\"\n",
    "    \n",
    "def extract_json(text):\n",
    "    # Remove unwanted tags like <think> and <speak>\n",
    "    cleaned_text = re.sub(r'<\\/?[\\w\\d]+>', '', text).strip()\n",
    "\n",
    "    # Now try to extract the JSON part using regex\n",
    "    match = re.search(r'\\{.*\\}', cleaned_text, re.DOTALL)\n",
    "    if not match:\n",
    "        raise ValueError(\"No JSON object found in response\")\n",
    "    return json.loads(match.group(0))\n",
    "\n",
    "def normalize_to_queries(output: str) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Normalize LLM output into a dict matching the Queries schema.\n",
    "    Always returns: {\"queries\": [...]}.\n",
    "    Also logs the result as clean JSON.\n",
    "    \"\"\"\n",
    "\n",
    "    # Remove <think>...</think> blocks if present\n",
    "    output = re.sub(r\"<think>.*?</think>\", \"\", output, flags=re.DOTALL).strip()\n",
    "\n",
    "    data: Dict[str, Any]\n",
    "\n",
    "    # Try strict JSON parse first\n",
    "    try:\n",
    "        parsed = json.loads(output)\n",
    "        if isinstance(parsed, dict) and \"queries\" in parsed:\n",
    "            data = parsed\n",
    "        elif isinstance(parsed, list):\n",
    "            data = {\"queries\": parsed}\n",
    "        else:\n",
    "            raise ValueError(\"Invalid schema\")\n",
    "    except Exception:\n",
    "        # Fallback: treat as markdown/bullet/numbered list\n",
    "        lines = [\n",
    "            re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"`')\n",
    "            for line in output.splitlines() if line.strip()\n",
    "        ]\n",
    "        # Deduplicate while preserving order\n",
    "        seen = set()\n",
    "        unique_lines = [q for q in lines if not (q in seen or seen.add(q))]\n",
    "        data = {\"queries\": unique_lines}\n",
    "\n",
    "    return data\n",
    "    \n",
    "class Queries(BaseModel):\n",
    "    queries: List[str]\n",
    "\n",
    "#implement nodes\n",
    "def plan_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=PLAN_PROMPT),\n",
    "        HumanMessage(content=state[\"task\"])\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\"plan\": response_content}\n",
    "    \n",
    "def research_plan_node(state: dict):\n",
    "    \"\"\"\n",
    "    Generates a research plan using a Qwen model and Tavily search.\n",
    "    Works without Pydantic.\n",
    "    \"\"\"\n",
    "    # Invoke Qwen model (plain text output)\n",
    "    raw_response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_PLAN_PROMPT),\n",
    "            HumanMessage(content=state[\"task\"]),\n",
    "        ]\n",
    "    )\n",
    "    response_content = getattr(raw_response, \"content\", str(raw_response))\n",
    "\n",
    "    # Remove <think>...</think> if present\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Normalize into a list of queries (handle bullets, numbers, etc.)\n",
    "    lines = [\n",
    "        re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"`')\n",
    "        for line in response_content.splitlines()\n",
    "        if line.strip()\n",
    "    ]\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    queries_list = [q for q in lines if not (q in seen or seen.add(q))]\n",
    "\n",
    "    # Initialize content\n",
    "    content = state.get(\"content\", [])\n",
    "\n",
    "    # Perform Tavily searches\n",
    "    for q in queries_list:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            for r in response.get(\"results\", []):\n",
    "                content_piece = r.get(\"content\", \"\")\n",
    "                if content_piece:\n",
    "                    content.append(str(content_piece))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "\n",
    "    return {\n",
    "        \"content\": content,\n",
    "    }\n",
    "def generation_node(state: AgentState):\n",
    "    content = \"\\n\\n\".join([\"content\"] or [])\n",
    "    user_message = HumanMessage(content=f\"{state['task']}\\n\\nHere is my plan:\\n\\n{state['plan']}\")\n",
    "    messages = [\n",
    "        SystemMessage(content=WRITER_PROMPT.format(content=content)),\n",
    "        user_message,\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "    return {\n",
    "        \"draft\": response_content,\n",
    "        \"revision_number\": state.get(\"revision_number\", 1) + 1,\n",
    "    }\n",
    "def reflection_node(state: AgentState):\n",
    "    messages = [\n",
    "        SystemMessage(content=REFLECTION_PROMPT),\n",
    "        HumanMessage(content=state['draft']),\n",
    "    ]\n",
    "    response = model.invoke(messages)\n",
    "    response_content = response.content if hasattr(response, 'content') else str(response)\n",
    "\n",
    "    # Remove <think>...</think> blocks completely\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    return {\"critique\": response_content}\n",
    "\n",
    "def research_critique_node(state: AgentState):\n",
    "    \"\"\"\n",
    "    Generates a research plan using a Qwen model and Tavily search.\n",
    "    Works without Pydantic.\n",
    "    \"\"\"\n",
    "    # Invoke Qwen model (plain text output)\n",
    "    raw_response = model.invoke(\n",
    "        [\n",
    "            SystemMessage(content=RESEARCH_CRITIQUE_PROMPT),\n",
    "            HumanMessage(content=state[\"critique\"]),\n",
    "        ]\n",
    "    )\n",
    "    response_content = getattr(raw_response, \"content\", str(raw_response))\n",
    "\n",
    "    # Remove <think>...</think> if present\n",
    "    response_content = re.sub(r\"<think>.*?</think>\", \"\", response_content, flags=re.DOTALL).strip()\n",
    "\n",
    "    # Normalize into a list of queries (handle bullets, numbers, etc.)\n",
    "    lines = [\n",
    "        re.sub(r'^\\s*[\\d\\-\\*\\.\\)]*\\s*', '', line).strip(' *\"`')\n",
    "        for line in response_content.splitlines()\n",
    "        if line.strip()\n",
    "    ]\n",
    "    # Deduplicate\n",
    "    seen = set()\n",
    "    queries_list = [q for q in lines if not (q in seen or seen.add(q))]\n",
    "\n",
    "    # Initialize content\n",
    "    content = state.get(\"content\", [])\n",
    "\n",
    "    # Perform Tavily searches\n",
    "    for q in queries_list:\n",
    "        try:\n",
    "            response = tavily.search(query=q, max_results=2)\n",
    "            for r in response.get(\"results\", []):\n",
    "                content_piece = r.get(\"content\", \"\")\n",
    "                if content_piece:\n",
    "                    content.append(str(content_piece))\n",
    "        except Exception as e:\n",
    "            print(f\"Search failed for query '{q}': {e}\")\n",
    "\n",
    "    return {\n",
    "        \"content\": content,\n",
    "    }\n",
    "\n",
    "def should_continue(state):\n",
    "    if state[\"revision_number\"] > state[\"max_revisions\"]:\n",
    "        return END\n",
    "    return \"reflect\"\n",
    "\n",
    "\n",
    "#build graph\n",
    "builder = StateGraph(AgentState)\n",
    "builder.add_node(\"planner\", plan_node)\n",
    "builder.add_node(\"generate\", generation_node)\n",
    "builder.add_node(\"reflect\", reflection_node)\n",
    "builder.add_node(\"research_plan\", research_plan_node)\n",
    "builder.add_node(\"research_critique\", research_critique_node)\n",
    "\n",
    "#set entry point\n",
    "builder.set_entry_point(\"planner\")\n",
    "\n",
    "#define conditional edges\n",
    "builder.add_conditional_edges(\n",
    "    \"generate\", \n",
    "    should_continue, \n",
    "    {END: END, \"reflect\": \"reflect\"}\n",
    ")\n",
    "\n",
    "#define edges\n",
    "builder.add_edge(\"planner\", \"research_plan\")\n",
    "builder.add_edge(\"research_plan\", \"generate\")\n",
    "\n",
    "builder.add_edge(\"reflect\", \"research_critique\")\n",
    "builder.add_edge(\"research_critique\", \"generate\")\n",
    "\n",
    "\n",
    "\n",
    "#print graph\n",
    "#Image(graph.get_graph().draw_png())\n",
    "\n",
    "\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "with SqliteSaver.from_conn_string(\":memory:\") as checkpointer:\n",
    "    graph = builder.compile(checkpointer=checkpointer)\n",
    "    for s in graph.stream({\n",
    "        'task': \"what is the difference between langchain and langsmith\",\n",
    "        \"max_revisions\": 2,\n",
    "        \"revision_number\": 1,\n",
    "    }, thread):\n",
    "        print(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1ba91136-e012-4996-aa8e-409a3f70bd87",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Running on local URL:  http://0.0.0.0:7860\n",
      "* Running on public URL: https://9c1fb780e711e9f802.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9c1fb780e711e9f802.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from helper import ewriter, writer_gui\n",
    "MultiAgent = ewriter()\n",
    "app = writer_gui(MultiAgent.graph)\n",
    "app.launch(share=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "572a74a3-9c14-41a5-9131-80bd7064f444",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     +-----------+                  \n",
      "                     | __start__ |                  \n",
      "                     +-----------+                  \n",
      "                            *                       \n",
      "                            *                       \n",
      "                            *                       \n",
      "                      +---------+                   \n",
      "                      | planner |                   \n",
      "                      +---------+                   \n",
      "                            *                       \n",
      "                            *                       \n",
      "                            *                       \n",
      "                   +---------------+                \n",
      "                   | research_plan |                \n",
      "                   +---------------+                \n",
      "                            *                       \n",
      "                            *                       \n",
      "                            *                       \n",
      "                      +----------+                  \n",
      "                      | generate |                  \n",
      "                   ...+----------+***               \n",
      "               ....         .        ****           \n",
      "           ....             .            ****       \n",
      "         ..                 .                ****   \n",
      "+---------+           +---------+                ** \n",
      "| __end__ |           | reflect |               **  \n",
      "+---------+           +---------+             **    \n",
      "                                ***         **      \n",
      "                                   *      **        \n",
      "                                    **   *          \n",
      "                            +-------------------+   \n",
      "                            | research_critique |   \n",
      "                            +-------------------+   \n"
     ]
    }
   ],
   "source": [
    "app.graph.get_graph().print_ascii()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5209d405-e9cf-427f-9ecc-99afc88c1815",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
